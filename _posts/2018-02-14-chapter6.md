---
layout: post
title: "ISL_Chapter6_Linear Model Selection and Regularization"
author: "hyeju.kim"
categories: facts
tags: [ISL]
image: 
---

# Chapter 6. Linear Model Selection and Regularization 

to improve simple linear model, not using least square methods

- prediction accuracy 

  if n>>p, low variance

  if n is not much larger than p, a lot of variability

  if p>n, infinite variance

  by constraining or shrinking the estimated coefficients, we can often substantaiially reduce the variance at the cost of a negligible increase in bias

- Model Interpretability

  excluding irrelevant variables from a multiple regression model through feature selection or variable selection



- Alternatives to least squares method

  1) Subset Selection

  2) Shrinkage

  3) Dimension Reduction



## 6.1 Subset Selection

### 6.1.1 Best Subset Selection

![image](https://user-images.githubusercontent.com/32008883/35909402-d5fcb7da-0c36-11e8-8aeb-bbfd6cac96bd.png)

- computational limitations

### 6.1.2 Stepwise Selection

### *Forward Stepwise Selection*

Unlike best subset selection, which involved fitting 2p models, forward stepwise selection involves fitting one null model, along with p − k models in the kth iteration, for k = 0, . . . , p − 1. This amounts to a total of $1 + \sum_{k=0}^{p-1} {p−k} = 1 + \frac{p(p+1)}{2}$ models. 



![image](https://user-images.githubusercontent.com/32008883/35909801-f743fefc-0c37-11e8-99fb-9be963f76b17.png)

- n < p



### *Backward Stepwise Selection*

![image](https://user-images.githubusercontent.com/32008883/35955378-5936dfea-0cd2-11e8-9c4c-d2acf15042cb.png)

- n > p



### *Hybrid Approaches*

forward selection + bacward elimination



### 6.1.3 Choosing the Optimal Model

- Two ways of estimating test error rate

  - indirectly estimate test error by making an adjustment to the training error to account for the bias due to overfitting

    - $C_p$ : *the lower, the better*

      ![image](https://user-images.githubusercontent.com/32008883/35955998-50725250-0cd6-11e8-8000-5b9ec4e8e741.png)

    - AIC : *the lower, the better*

      - proportional to $C_p$

      ![image](https://user-images.githubusercontent.com/32008883/35956042-9010f04c-0cd6-11e8-845d-86f9e7835947.png)

    - BIC: *the lower, the better*

      - a heavier penalty on models with many variables

      ![image](https://user-images.githubusercontent.com/32008883/35956090-e0a7553c-0cd6-11e8-9b84-3d99a06bddef.png)

      ​

    - Adjusted $R^2$ : *the higher, the better*

      ![image](https://user-images.githubusercontent.com/32008883/35956147-35a68c74-0cd7-11e8-89dc-5c57353e8113.png)

      ​

      ​

  - directly estimate the test error, using either a validation set approach or a cross-validation approach

    - Validation and Cross-Validation

      - computational development -> possible

      - if flat -> repeat -> would change

      - *one-stamdard-error rule*

        first, calculate the standard error of the estimated test MSE for each model size, and then select the smallest model for which the estimated test error is within one stardarad error of the lowest point on the curve 

        if more or less equally good, choose the simplest model





