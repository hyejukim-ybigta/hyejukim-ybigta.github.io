---
layout: post

title: "ISL_Chapter9_Support Vector Machines"

author: "hyeju.kim"

categories: ML

tags: [ISL]

image: 
---



## 9.1 Maximal Margin Classifier

### 9.1.1 What Is a Hyperplane?

- in a p-dimensional space, a hyperplane is a flat affine subspace of dimension p-1

ex) in two dimensions, a hyperplane is a line $\beta_0 + \beta_1X_1 + \beta_2X_2 = 0$

- divide p-dimensional space into two halves(according to >0 or <0)

### 9.1.2 Classification Using a Separating Hyperplane

![image](https://user-images.githubusercontent.com/32008883/36377130-3f1e4860-15b9-11e8-8e70-982891d4cd4d.png)

![image](https://user-images.githubusercontent.com/32008883/36377156-5a4b362a-15b9-11e8-84f2-6e30c7fff329.png)

- a test observation is assigned a class depending on which side of the hyperplane it is located


- magnitude of $f(x^*) = \beta_0 + \beta_1x_1^* + \beta_2x_2^* + ... +\beta_px_p^*$
  -  if far from xero, confident

### 9.1.3 The Maximal Margin Classifier

-  seperating hyperplane for which the margin is largest- that is, it is the hyperplane that has the farthest minimum distance to the training observations. 
-  margin : the perpendicular distance from each training observation to a given seperating hyperplane; the smallest such distance is the minimal distance from the observations to the hyperplane, and is known as the margin
-  can lead to overfitting when p is large
-  if $\beta_0, \beta_1, ... , \beta_p$ are the coefficients of the maximal margin hyperplane, then the maximal margin classifier classifies the test observation $x^*$ based on the sign of $f(x^*)$
-  support vector : they "support" the maximal margin hyperplane in the sense that if these points wer moved slightly then the maximal margin hyperplane would move as well.

![image](https://user-images.githubusercontent.com/32008883/36379539-5a8b1ab6-15c2-11e8-9902-d52b51d4f9be.png)



### 9.1.4 Construction of the Maximal Margin Classifier

- optimization?

![image](https://user-images.githubusercontent.com/32008883/36379334-79ca3b6a-15c1-11e8-8fb7-3e317ce91b07.png)



- - (9.11) : each observation will be on the correct side of the hyperplane, provided that M is positive
  - (9.10) : with this constraints the perpendicular distance from the ith observation to the hyperplane is given by  $y_i(\beta_0 + \beta_1x_{i1} + \beta_2x_{i2} + ... +\beta_px_{ip}$
  - (9.10)&(9.11): each observation is on the correct side of the hyperplane and at leasta a distance M from the hyperplane
  - M : the margin of our hyperplane
  - (9.9) : the optimization problem chooses $\beta_0, \beta_1, ... , \beta_p$ to maximize M



### 9.1.5 The Non-separable Case

- problem arises when *if a separating hyperplane exists*
  - find a hyperplane that almost separtes the classes, using *soft margin* = support vector classifier



## 9.2 Support Vector Classifiers

### 9.2.1 Overview of the Support Vector Classifier

- why support vector classifier?
  - greater robustness to individual observations
  - better classification of most of the training observations
- allow some observations to be on the incorrect side of the margin, or even the incorrect side of the hyperplane(The margin is *soft* because it can be violated by some of the training observations)



## 9.2.2 Details of the Support Vector Classifier

![image](https://user-images.githubusercontent.com/32008883/36407065-db26b51e-163e-11e8-931c-d152f7f9df1a.png)

- - (9.14) : $\epsilon_1,...,\epsilon_n$ are *slack variables* that allow individual observations to be on the wrong side of the margin or the hyperplane. slack variable $\epsilon_i$ tells us where the $i$th observation is located, relative to the hyperplane and relative to the margin. 

    -  if $\epsilon_i = 0$, then the $i$th observation is on the correct side of the margin
    -  if $\epsilon_i > 0$, then the $i$th observation is on the wrong side of the margin
    -  if $\epsilon_i > 1$, then the $i$th observation is on the wrong side of the hyperplane

    ?? *slack variable research needed*

  - (9.14): the tuning parameter $C$ determines the number and severity of the violations to the margin(and to the hyperplane) that we will tolerate =  a budget for the amount that the margin can be violated by the n observations

    - if $C=0$ , simply amounts to the maximal margin hyperplane optimization
    - if $C>0$, no more than $C$ observations can be on the wrong side of the hyperplane
    - the budget $C$ increases, we become more tolerant of violations to the margin, and so the margin will widen
    - $C$ chosen by cross-validation
    - bias-variance trade-off :
      -  if C is small, low bias and high variance
      -  if C is large, high bias and low variance

  - (9.12)-(9.15): only obervations that either lie on the margin or that violate the margin (*support vectors*)will affect the hyperplane, and hence the classifier obtatined. = an observation that lies strictly on the correct side of the margin does not affect the support vector lcassifier

    - if $C$ is large, margin is wide, so many support vectors $\to$ low variance, high bias
    - if $C$is samll, fewer support vectors $\to$ low bias, high variance
    - decision rule is only based on the support vectors means that it is quite robust to the behavior of observations that are far away from the hyperplane. 

    ![image](https://user-images.githubusercontent.com/32008883/36407468-380d0cbc-1642-11e8-8d5a-3e099f7e52db.png)

    ![image](https://user-images.githubusercontent.com/32008883/36407476-47edf236-1642-11e8-8a3a-bec00f402398.png)

    ​

## 9.3 Support Vector Machines

- non-linear boundaries

### 9.3.1 Classification with Non-linear Decision Boundaries

- enlarging the feature space using quadratic, cubic, and even higher-order polynomial functions of the predictors
  - ex) fit a support vector classifier using 2p features( $X_1,X_1^2,X_2,X_2^2, ... , X_p, X_p^2$)
  - Then (9.12)–(9.15) would become
  - ![image](https://user-images.githubusercontent.com/32008883/36463122-1b7bb912-170b-11e8-92a4-4009f2e6f82e.png)
  - ??? But in the original feature space, the decision boundary is of the form q(x) = 0, where q is a quadratic polynomial, and its solutions are generally non-linear
  - other functions of the predictors can be used to enlarge the feature space, but be careful of a huge number of features



### 9.3.2 The Support Vector Machine

- an extension of the support vector classifier that results from enlarging the feature space in a specific way, using *kernels*.
- ??? However, it turns out that the solution to the support vector classifier problem (9.12)–(9.15)
  involves only the inner products of the observations (as opposed to the observations themselves). The inner product of two r-vectors a and b is defined as 
- ![image](https://user-images.githubusercontent.com/32008883/36463295-6acf8312-170c-11e8-90bd-bc5ccc32d383.png)
- ![image](https://user-images.githubusercontent.com/32008883/36463359-c0d8397a-170c-11e8-8bf8-b70781241bc0.png)
- ![image](https://user-images.githubusercontent.com/32008883/36467879-25cc2cc6-1724-11e8-9944-58d28556f646.png)
- ​
- http://ifyouwanna.tistory.com/entry/%EB%82%B4%EC%A0%81%EC%9D%98-%ED%99%9C%EC%9A%A9



- $K(x_i, x_{i^`})$

  - kernel : a generalizaiton of the inner product, where K is some function that we will refer to as a kernel

  - a function that quantifies the similarity of two observations.

  - a linear kernel (Pearson (standard) correlation):

  - ![image](https://user-images.githubusercontent.com/32008883/36468104-641927d0-1725-11e8-8581-a0ab01538a71.png)

  - polynomial kernel(flexible. non-linear):

    - d:positive integer

  - ![image](https://user-images.githubusercontent.com/32008883/36468094-5318c418-1725-11e8-999d-765c83f1346a.png)

  - radial kernel(non-linear):

    - $\gamma$ : positive constant

    - ![image](https://user-images.githubusercontent.com/32008883/36468581-3c7f95ea-1727-11e8-92a1-fc21b752e929.png)

    - ​

    - if a given test observation $x^* = (x^*_1 ... x^*_p)^T$ is far from a training observation $x_i $in terms of Euclidean distance, then $\sum_{j=1}^p(x^*_j - x_{ij})^2$ will be large, and so (9.24) will be very tiny. This means that in (9.23), $x_i$ will play virtually no role in $f(x^*)$. training observations that are far from $x^*$ will play essentially no role in the predicted class label for $x^*$

    - the radial kernel has very *local* behavior, in the sense that only nearby training observations have an effect on the class label of a test observation.

    - ​

      x∗ = (x∗
      1 . . .x∗
      p)T is far from a training observation xi in terms of
      Euclidean distance, then


      p
      j=1(x∗
      j
      −xij)2 will be large, and so K(xi, xi) =
      exp(−γ


      p
      j=1(x∗
      j
      − xij )2) will be very tiny.

  - ![image](https://user-images.githubusercontent.com/32008883/36468357-7c715ce8-1726-11e8-8385-0ff0575c7698.png)

    ![image](https://user-images.githubusercontent.com/32008883/36468484-dafc275c-1726-11e8-8de2-7c950e2287ee.png)

  - left - polynomial // right -radial

- SVM

  - When the support vector classifier is combined with a non-linear kernel such as (9.22), the resulting classifier is known as a support vector machine.
  - ![image](https://user-images.githubusercontent.com/32008883/36468162-9c73bf14-1725-11e8-9e48-b3887f91dcf3.png)
  - ​

- ​

- ​