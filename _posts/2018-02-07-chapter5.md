---
layout: post
title: "ISL_Chapter5_Resampling Methods"
author: "hyeju.kim"
categories: facts
tags: [ISL]
image: CrossValidation.png
---

# Chapter 5. Resampling Methods

## 5.1 Cross-Validation

### 5.1.1 The Validation Set Approach

- **training set** and **validation set**
- validation set error rate(typically MSE on quantitative)

![image](https://user-images.githubusercontent.com/32008883/35902740-b5ddc2b2-0c1f-11e8-8f6a-c9a0a8d4a80d.png)



drawback 1 : validation estimate of the test error rate can be highly variable

drawback 2 : In the validation approach, only a subset of the observations, so the validation set error rate may tend to overestimate the test error rate for the model fit on the entire data set





### 5.1.2 Leave-One-Out Cross-Validation

![image](https://user-images.githubusercontent.com/32008883/35902723-a23ece9a-0c1f-11e8-9373-026d30a9fbe6.png)

a single observation (x1, y1) is used for the validation set, and the remaining observations {(x2, y2), . . . , (xn, yn)} make up the training set. -> MSE1

MSE2 = (y2−ˆy2)2.

![image](https://user-images.githubusercontent.com/32008883/35902797-f2a105ec-0c1f-11e8-8bd5-840d3fc85d79.png)

- *the LOOCV estimate for the test MSE*: the average of these n test error estimates

advantage1 : far less bias (not to overestimate)

advantage2: always yield the same results

if n is large : use the following formula(not always)

![image](https://user-images.githubusercontent.com/32008883/35903172-631e1796-0c21-11e8-9aff-b3133339f875.png)



### 5.1.3 k-Fold Cross-Validation

- LOOCV is a special case of k-fold CV

- randomly dividing the set of observations into k groups, or folds, of approximately equal size. The first fold is treated as a validation set, and the mehtod is fit on the remaining k-1 folds

  ![image](https://user-images.githubusercontent.com/32008883/35903474-b35c9222-0c22-11e8-85ae-b44c4e0e3426.png)

  ![image](https://user-images.githubusercontent.com/32008883/35903510-de3eb858-0c22-11e8-850a-1b863f92a6ba.png)

  - k = 5, k = 10 outputs would be similar to LOOCV
  - important to see *the smallest test MSE* than identify the correct level of flexibility



### 5.1.4 Bias-Variance Trade-Off for k-Fold Cross-Validation

- why k-fold CV >> LOOCV?
  - computational advantage
  - bias-variance trade-off
    - bias : LOOCV << CV
    - variance : LOOCV(highly correlated) >> CV

### 5.1.5 Cross-Validation on Classification Problems

- instead of MSE, number of misclassified observations

  ![image](https://user-images.githubusercontent.com/32008883/35904496-a97a9ea8-0c26-11e8-85f5-4dff9b40f2ed.png)

  where $E_{rri} = I(y_i \neq \hat{y_i})$



## 5.2 The Bootstrap

- to quantify *the uncertainty* associated with a given estimator or statistical learning method
- rather than repeatedly obtaining independent data sets from the population, instead obtain distinct data sets by repeatedly samply observations from the original data set.



![image](https://user-images.githubusercontent.com/32008883/35905389-45edba92-0c2a-11e8-9f17-1ebe7053cc02.png)

![image](https://user-images.githubusercontent.com/32008883/35905417-62afc7ba-0c2a-11e8-9a9c-038785f90a08.png)

