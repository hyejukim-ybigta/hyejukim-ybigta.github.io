---
layout: post
title: "1st sample"
author: "Hyeju Kim"
categories: sample
tages: [sample]
image: ![sample image](https://user-images.githubusercontent.com/32008883/34977620-99145b0c-fade-11e7-9b0e-c4398d95f9a2.jpg)
---



![sample image](https://user-images.githubusercontent.com/32008883/34977620-99145b0c-fade-11e7-9b0e-c4398d95f9a2.jpg)





##  CS231n 4강(혜주)

다음은 스탠포드 대학 강의  [CS231n: Convolutional Neural Networks for Visual Recognition](http://cs231n.stanford.edu/) 을 요약한 것입니다.



4강의 내용을 도식화해서 정리하면 다음과 같습니다. 

![cs231n_week4_diagram](https://user-images.githubusercontent.com/32008883/31546726-74d4b67a-b05f-11e7-98d9-6bd38e1bfa3f.JPG)

##  1. Back Propagation

3강에서 배웠던 optimization을 위해서는 gradient를 계산해야 합니다.그래서 이번 강의에서는 computational graph 에서 gradient를 계산하는 것을 배웁니다. input에 따른 W값,ouput 이 계산되면, 다시 앞으로 오면서 gradient를 계산합니다. 이 gradient를 참고해서 optimization이 이루어집니다.

###1) chain rule을 통한 gradient 계산

하단의 예제를 통해 이해해봅시다!

![cs231n_week4_1](https://user-images.githubusercontent.com/32008883/31546815-e6524fba-b05f-11e7-9e5c-0ace579f3028.JPG)



다음과 같은 식이 있다고 할 때, 하단의 gradient를 구하는 것은 chain rule을 사용하는 것입니다.

하단의 computer graph에서 output 쪽에 gradient는 1입니다.

![cs231n_week4_2](https://user-images.githubusercontent.com/32008883/31546831-f87d6774-b05f-11e7-8d5f-f6c6ff2c78b4.JPG)

![cs231n_week4_3](https://user-images.githubusercontent.com/32008883/31546856-1458c02e-b060-11e7-95bc-6696963b531a.JPG)

f를 z로 미분하면 q이고 q = x+y이기 때문에 gradient는 3입니다.

![cs231n_week4_4](https://user-images.githubusercontent.com/32008883/31546868-25dceb4a-b060-11e7-9bf0-3f3c1930ad31.JPG)

f를 q로 미분하면 z이고 gradient는 -4입니다.

![cs231n_week4_5](https://user-images.githubusercontent.com/32008883/31546990-99473220-b060-11e7-957a-3f8507416458.JPG)

f를 y로 미분한 값은 chain rule을 통해 f의 q에 대한 미분과 q의 y에 대한 미분을 곱하면 얻을 수 있습니다. 즉 z*1 = -4입니다.

![cs231n_week4_6](https://user-images.githubusercontent.com/32008883/31546991-99760906-b060-11e7-8d81-fc63dd928f77.JPG)

마찬가지로 f를 x로 미분한 값은 chain rule을 통해 f의 q에 대한 미분과 q의 x에 대한 미분을 곱하면 얻을 수 있습니다. 즉 z*1 = -4입니다.



이를 간단히 도식화하면

![cs231n_week4_7](https://user-images.githubusercontent.com/32008883/31547074-f0cc47ba-b060-11e7-96f9-f68fc413ba51.JPG)

input을 통해 gradient를 얻게 되고, 연결된 바로 직전의 node에 gradient를 전달하게 됩니다. gradient는 local gradient와 upstream gradient를 구하여 이루어집니다.



더 복잡한 예제도 결국 chain rule을 통해서 구할 수 있습니다.(자세한 풀이는 생략)

![cs231n_week4_8](https://user-images.githubusercontent.com/32008883/31547075-f104bd48-b060-11e7-8fdf-ddb4c1ff1846.JPG)

![cs231n_week4_9](https://user-images.githubusercontent.com/32008883/31547076-f13d7b74-b060-11e7-8d18-aa365d96363d.JPG)

- chain rule을 통한 gradient 계산의 장점

  어떠한 복잡한 node도 group화 시켜서 big node로 바꿀 수 있습니다. 예를 들어 sigmoid function의 경우, 미분하면 하단과 같은 형태로 나타나기 때문에 계산하면 0.2로 나타납니다. 즉 한번의 gradient 계산만 있으면 됩니다.



![cs231n_week4_10](https://user-images.githubusercontent.com/32008883/31547077-f1734cfe-b060-11e7-984b-ed613452b6a1.JPG)

- vector로 확장

  ![cs231n_week4_11](https://user-images.githubusercontent.com/32008883/31547332-e55f5628-b061-11e7-8e7f-f07877a47868.JPG)

  ​

vector로 확장하면 jacobian matrix가 필요한데, 이것은 local gradient와 동일하다고 볼 수 있습니다. 

![cs231n_week4_12](https://user-images.githubusercontent.com/32008883/31547334-e5a380dc-b061-11e7-885b-16059dfa846c.JPG)

그렇다면 input과 output이  4096 dimension인 경우 jacobian matrix는 어떠한 형태를 가질까요? 4096x4096 형태라고 볼 수 있습니다. 그러나 minibatch가 있는 경우 더 큰 크기의 행렬일 수 있습니다. 

vector 형태로 gradient를 계산하는 것은 하단과 같은 결과를 보여줍니다. (과정은 복잡해서 생략..)

![cs231n_week4_16](https://user-images.githubusercontent.com/32008883/31547479-7460e0e4-b062-11e7-829a-b21c261ed5d7.JPG)




- forward/backward API

  ![cs231n_week4_18_2](https://user-images.githubusercontent.com/32008883/31547808-a20f3f94-b063-11e7-98c8-eb5788103ef2.jpg)

  상단과 같은 computational graph를 가진 경우 forward 와 backward를 계산하는 코드는 다음과 같습니다.

  ```python
  class MultiplyGate(object):
    def forward(x,y):
      z= x*y
      self.x = x
      self.y = y
      return z
    def backward(dz):
      dx = self.y + dz #[dz/dx + dL/dz]
      dy = self.x + dz #[dz/dy + dL/dz]
      return [dx,dy]
  ```

  ​

  ![cs231n_week4_19](https://user-images.githubusercontent.com/32008883/31547837-c7c5cafa-b063-11e7-9d62-c9e01de18b09.JPG)

  ​

## 2. Neural Networks 소개

그 전에는 linear한 score function하나만으로 계산했다면, Neural Networks는 W1,W2,W3..등 여러개의 layer를 가집니다. 그 사이에는 nonlinear한 function들로 연결됩니다. 그 예시들은 뒤의 강의에배우게 됩니다.

예를 들어 W1은 input과 직접적으로 관련된 템플릿이지만, W2는 score를 적절히 결합한 템플릿이라고 볼 수 있습니다.



![cs231n_week4_20](https://user-images.githubusercontent.com/32008883/31547838-c7f05e96-b063-11e7-8517-005d47a0395e.JPG)



- 2 layer의 Neural Network의 코드

  ```python
  import numpy as np
  from numpy.random import randn

  N, D_in, H, D_out = 64, 1000, 100, 10
  x, y = randn(N, D_in), randn(N, D_out)
  w1, w2 = randn(D_in, N), randn(N, D_out)

  for t in range(2000):
    h = 1 / (1 + np.exp(-x.dot(w1)))
    y_pred = h.dot(w2)
    loss = np.square(y_pred - y).sum()
    print(t, loss)
    
    grad_y_pred = 2.0 * (y_pred - y)
    grad_w2 = h.T.dot(grad_y_pred)
    grad_h = grad_y_pred.dot(w2.T)
    grad_w1 = x.T.dot(grad_h * h * (1 - h))
    
    w1 -= 1e-4 * grad_w1
    w2 -= 1e-4 * grad_w2
    
  ```





- 'Neural Network' 이름의 유래

  neuron의 자극 처리 메커니즘과 neural network의 메커니즘이 유사한 부분이 있기 때문입니다.

  input을 자극으로 생각하고 , 데이터를 처리하는 함수가 cell body로 볼 수 있습니다. activation function을 통해 결과값이 나오는 것도 유사하게 볼 수 있습니다. 하지만 정확히 뉴런의 메커니즘과 같지는 않으니 참고만 하면 됩니다.



![cs231n_week4_22](https://user-images.githubusercontent.com/32008883/31547840-c84e3822-b063-11e7-92de-9652e5f86d34.JPG)

![cs231n_week4_23](https://user-images.githubusercontent.com/32008883/31547841-c8789c84-b063-11e7-9158-8562461cb2aa.JPG)

